{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acting-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import Counter\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import getopt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sys import platform as _platform\n",
    "\n",
    "if _platform == \"win32\":\n",
    "    sys.path.insert(1, '..\\..\\\\')\n",
    "else:\n",
    "    sys.path.insert(1, '../../')\n",
    "from utils import fileutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fantastic-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, hidden_size):\n",
    "        \n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size,\n",
    "                            hidden_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_size, n_vocab)\n",
    "    \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.gru(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "#         print('zero_state called')\n",
    "        return Variable(torch.zeros(1,batch_size,self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "amateur-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "for_units = True\n",
    "weights, weights_units, weight_tensor = [], [], None\n",
    "len_dict = {}\n",
    "\n",
    "# options, remainder = getopt.getopt(sys.argv[1:], 'e:l:u')\n",
    "# for opt, arg in options:\n",
    "#     if opt in ('-e'):\n",
    "#         try:\n",
    "#             epochs = int(arg.strip())\n",
    "#         except ValueError:\n",
    "#             sys.exit('Epoch has to be a positive integer.')\n",
    "#     elif opt in ('-l'):\n",
    "#         try:\n",
    "#             learning_rate = float(arg.strip())\n",
    "#         except ValueError:\n",
    "#             sys.exit('Learning Rate has to be a numeric value.')\n",
    "#     elif opt in ('-u'):\n",
    "#         for_units = True\n",
    "\n",
    "epochs = 150\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dirty-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_list(dataframe_obj):\n",
    "    '''\n",
    "    Method to return list of all the values in a single row separated by spaces from the dataframe.\n",
    "    All values that were space separated before are converted to a single word.\n",
    "    example. Sea Surface Temperature -> SeaSurfaceTemperature\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe_obj : pandas dataframe\n",
    "        Dataframe contains the training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_list : list\n",
    "        List of input sentences.\n",
    "    reference_dict : dict\n",
    "        Mapping of the word to its space-stripped version used for training.\n",
    "\n",
    "    '''\n",
    "    reference_dict = {}\n",
    "    \n",
    "    dataframe_obj = dataframe_obj.replace(np.nan, 'NA', regex=True)\n",
    "    lipd_data_list = dataframe_obj.values.tolist()\n",
    "\n",
    "    new_list = []\n",
    "    for lis in lipd_data_list:\n",
    "        for val in lis:\n",
    "            reference_dict[val] = val.replace(\" \", \"\")\n",
    "        lis = [val.replace(\" \", \"\") for val in lis]\n",
    "        lis = (',').join(lis)\n",
    "        new_list.append(lis)\n",
    "\n",
    "    return new_list, reference_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "residential-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unique_chains(dataframe_obj):\n",
    "\n",
    "    global weights, weights_units\n",
    "\n",
    "    chain1 = dataframe_obj[['archiveType', 'proxyObservationType','units']]\n",
    "    \n",
    "    chain2 = dataframe_obj.filter(['archiveType', 'proxyObservationType', 'interpretation/variable', 'interpretation/variableDetail', \n",
    "    'inferredVariable', 'inferredVarUnits'])\n",
    "    \n",
    "\n",
    "    chain1_list = chain1.values.tolist()\n",
    "\n",
    "    for lis in chain1_list:\n",
    "        lis = [val.replace(\" \", \"\") for val in lis]\n",
    "        lis = (',').join(lis)\n",
    "        \n",
    "        com_ind = lis.find(',', lis.index(','))\n",
    "        while com_ind != -1:\n",
    "            weights.append(lis[:com_ind])\n",
    "            com_ind = lis.find(',', com_ind+1)\n",
    "        weights_units.append(lis)\n",
    "\n",
    "    chain2_list = chain2.values.tolist()\n",
    "    for lis in chain2_list:\n",
    "        lis = [val.replace(\" \", \"\") for val in lis]\n",
    "        lis = (',').join(lis)\n",
    "        \n",
    "        com_ind = lis.find(',', lis.index(','))\n",
    "        while com_ind != -1:\n",
    "            weights.append(lis[:com_ind])\n",
    "            com_ind = lis.find(',', com_ind+1)\n",
    "        weights.append(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "associate-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_df(lipd_data_df, batch_size, seq_size):\n",
    "    '''\n",
    "    Read training data into dataframe for training the model.\n",
    "    The training data needs to be Label Encoded because pytorch only works with float data.\n",
    "    Select only num_batches*seq_size*batch_size amount of data to work on.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lipd_data_df : pandas dataframe\n",
    "        Dataframe containing either training sdata.\n",
    "    batch_size : int\n",
    "        Used to divide the training data into batches for training.\n",
    "    seq_size : int\n",
    "        Defines the sequence size for the training sentences.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int_to_vocab : dict\n",
    "        Mapping of the Label Encoding int to text.\n",
    "    vocab_to_int : dict\n",
    "        Mapping of the Label Encoding text to int.\n",
    "    n_vocab : int\n",
    "        Size of the Label Encoding Dict.\n",
    "    in_text : list\n",
    "        Contains the input text for training.\n",
    "    out_text : list\n",
    "        Corresponding output for the input text.\n",
    "    reference_dict : dict\n",
    "        Mapping of the word to its space-stripped version used for training.\n",
    "\n",
    "    '''\n",
    "    global len_dict, weight_tensor\n",
    "\n",
    "    calculate_unique_chains(lipd_data_df)\n",
    "\n",
    "    if for_units:\n",
    "        lipd_data = lipd_data_df.filter(['archiveType', 'proxyObservationType', 'units'])\n",
    "    else:\n",
    "        lipd_data = lipd_data_df.filter(['archiveType', 'proxyObservationType', 'interpretation/variable', 'interpretation/variableDetail', 'inferredVariable', 'inferredVarUnits'])\n",
    "    new_list, reference_dict = convert_dataframe_to_list(lipd_data)\n",
    "    \n",
    "    token_list = (',').join(new_list)\n",
    "\n",
    "    text = token_list.split(',')\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "\n",
    "    weights_counter = Counter(weights)\n",
    "    weights_u_counter = Counter(weights_units)\n",
    "    \n",
    "\n",
    "    len_dict = {'1':[], '2':[], '3':[], '3_units':[], '4':[], '5':[], '6':[]}\n",
    "    for k,v in weights_counter.items():\n",
    "        k_l = k.split(',')\n",
    "        len_dict[str(len(k_l))].append(k)\n",
    "    \n",
    "    for k,v in weights_u_counter.items():\n",
    "        len_dict['3_units'].append(k)\n",
    "\n",
    "    l =  {k:len(v) for k,v in len_dict.items()}\n",
    "    print('list of len', l)\n",
    "\n",
    "    weights_counter.update(weights_u_counter)\n",
    "    print(len(weights_counter))\n",
    "    total_count = sum(weights_counter.values())\n",
    "    for k,v in weights_counter.items():\n",
    "        weights_counter[k] = v/total_count\n",
    "\n",
    "    weight_tensor = torch.FloatTensor(list(weights_counter.values())).cuda()\n",
    "\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, reference_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "uniform-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    '''\n",
    "    Returns a batch each for the input sequence and the expected output word.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_text : list\n",
    "        Label Encoded strings of text.\n",
    "    out_text : list\n",
    "        Label Encoded Output for each each input sequence.\n",
    "    batch_size : int\n",
    "        Parameter to signify the size of each batch.\n",
    "    seq_size : int\n",
    "        Parameter to signify length of each sequence. In our case we are considering 2 chains, one of length 3 and the other of length 6.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    list\n",
    "        batch of input text sequence each of seq_size.\n",
    "    list\n",
    "        batch of output text corresponding to each input.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    # Increment the loop by seq_size, because we have unique sequence and not a continuation.\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "approved-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.01):\n",
    "    '''\n",
    "    We are using CrossEntropy as a Loss Function for this RNN Model since this is a Multi-class classification kind of problem.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : neural network instance\n",
    "        Loss function is set for the Neural Network.\n",
    "    lr : float, optional\n",
    "        Defines the learning rate for the neural network. The default is 0.001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    criterion : Loss function instance\n",
    "        Loss Function instance for the neural network.\n",
    "    optimizer : Optimizing function instance\n",
    "        Optimizer used for the neural network.\n",
    "\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss(weight = None)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proprietary-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_save_loss_curve(train_loss_list, chain):\n",
    "    '''\n",
    "    Method to save the plot for the training loss curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_value_list : list\n",
    "        List with the training loss values.\n",
    "    chain : str\n",
    "        To differentiate between the proxyObservationTypeUnits chain from the proxyObservationType & interpretation/variable chain.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    plt.plot(train_loss_list, label='Train Loss')\n",
    "        \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    fig = plt.gcf()\n",
    "    if not sys.stdin.isatty():    \n",
    "        plt.show()\n",
    "        \n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M%S\")    \n",
    "    final_path = loss_curve_path + chain + '_gru_training_loss_e_' + str(epochs) + '_l_'+ str(learning_rate) + '_' + timestr + '.png'\n",
    "    print('\\nSaving the plot at ', final_path)\n",
    "    fig.savefig(final_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subtle-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(train_label_dict, seq_size):\n",
    "    '''\n",
    "    Method to train a gru model on in_text and out_text.\n",
    "    This method will save the model for the last epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_label_dict : dict\n",
    "        Contains training information like the encoded input sequence, encoded output sequence information and number of words for training and validation data.\n",
    "    seq_size : int\n",
    "        Parameter to signify length of each sequence. In our case we are considering 2 chains, one of length 3 and the other of length 6.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    in_text = train_label_dict['in_text']\n",
    "    out_text = train_label_dict['out_text']\n",
    "    n_vocab = train_label_dict['n_vocab']\n",
    "    \n",
    "    net = RNNModule(n_vocab, seq_size,\n",
    "                    flags.embedding_size, flags.hidden_size)\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion, optimizer = get_loss_and_train_op(net, learning_rate)\n",
    "\n",
    "    iteration = 0\n",
    "    train_loss_list = []\n",
    "    \n",
    "    if for_units:\n",
    "        print('\\nTraining Data for the chain archiveType -> proxyObservationType -> proxyObservationTypeUnits\\n')\n",
    "    else:\n",
    "        print('\\nTraining Data for the chain archiveType -> proxyObservationType -> interpretation/variable -> interpretation/variableDetail -> inferredVariable -> inferredVarUnits\\n')\n",
    "        \n",
    "    for e in range(epochs):\n",
    "\n",
    "        # TRAIN PHASE\n",
    "        batches = get_batches(in_text, out_text, flags.batch_size, seq_size)\n",
    "        state_h = net.zero_state(flags.batch_size)\n",
    "        \n",
    "        # Transfer data to GPU\n",
    "        state_h = state_h.to(device)\n",
    "        \n",
    "        train_loss = 0\n",
    "        iteration = 0\n",
    "        for x, y in batches:\n",
    "            iteration += 1\n",
    "            \n",
    "            # Tell it we are in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = torch.tensor(x).to(device).long()\n",
    "            y = torch.tensor(y).to(device).long()\n",
    "\n",
    "            logits, (state_h) = net(x, state_h)\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Perform back-propagation\n",
    "            loss.backward()\n",
    "            \n",
    "            _ = torch.nn.utils.clip_grad_norm_(\n",
    "                net.parameters(), flags.gradients_norm)\n",
    "\n",
    "            # Update the network's parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss_list.append(train_loss/iteration)\n",
    "\n",
    "        print('Epoch: {}/{}'.format(e, epochs),\n",
    "                'Training Loss: {}'.format(train_loss_list[-1]))  \n",
    "\n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M%S\")    \n",
    "    if for_units:\n",
    "        print('\\nSaving the model file...')\n",
    "        torch.save(net.state_dict(),model_file_path + 'model_gru_units_'+timestr+'.pth')\n",
    "    else:\n",
    "        print('\\nSaving the model file...')\n",
    "        torch.save(net.state_dict(),model_file_path + 'model_gru_interp_'+timestr+'.pth')\n",
    "    \n",
    "    print_save_loss_curve(train_loss_list, 'proxy_units_' if for_units else 'proxy_interp_')\n",
    "    print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "going-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    global int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, device\n",
    "    global int_to_vocab_u, vocab_to_int_u, n_vocab_u, in_text_u, out_text_u\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    train_df = pd.read_csv(flags.train_file)\n",
    "    train_df = train_df.replace(np.nan, 'NA', regex=True)\n",
    "\n",
    "    if not for_units:\n",
    "        int_to_vocab, vocab_to_int, n_vocab, in_text, out_text, reference_dict = get_data_from_df(\n",
    "            train_df, flags.batch_size, flags.seq_size)\n",
    "        \n",
    "        model_tokens = {'model_tokens' : int_to_vocab, 'reference_dict': reference_dict, 'len_dict' : {k:len(v) for k,v in len_dict.items()}}\n",
    "        with open(model_file_path+'model_token_info_'+timestr+'.txt', 'w') as json_file:\n",
    "            json.dump(model_tokens, json_file)\n",
    "\n",
    "        train_label_dict = {'n_vocab' : n_vocab, 'in_text' : in_text, 'out_text' : out_text}\n",
    "        # Train for archive -> proxyObservationType -> interpretation/variable -> interpretation/variableDetail -> inferredVariable -> inferredVarUnits\n",
    "        train_RNN(train_label_dict, flags.seq_size)\n",
    "    else:\n",
    "        int_to_vocab_u, vocab_to_int_u, n_vocab_u, in_text_u, out_text_u, reference_dict_u = get_data_from_df(\n",
    "            train_df, flags.batch_size, flags.seq_size_u)\n",
    "        \n",
    "        model_tokens = {'model_tokens_u' : int_to_vocab_u, 'reference_dict_u' : reference_dict_u, 'len_dict' : {k:len(v) for k,v in len_dict.items()}}\n",
    "        \n",
    "        with open(model_file_path+'model_token_units_info_'+timestr+'.txt', 'w') as json_file:\n",
    "            json.dump(model_tokens, json_file)\n",
    "        train_label_dict = {'n_vocab' : n_vocab_u, 'in_text' : in_text_u, 'out_text' : out_text_u}\n",
    "        # Train for archive -> proxyObservationType -> units\n",
    "        train_RNN(train_label_dict, flags.seq_size_u)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "inner-yemen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of len {'1': 14, '2': 112, '3': 147, '3_units': 160, '4': 243, '5': 459, '6': 465}\n",
      "1563\n",
      "\n",
      "Training Data for the chain archiveType -> proxyObservationType -> interpretation/variable -> interpretation/variableDetail -> inferredVariable -> inferredVarUnits\n",
      "\n",
      "Epoch: 0/150 Training Loss: 1.1902371436050259\n",
      "Epoch: 1/150 Training Loss: 0.8450263150778833\n",
      "Epoch: 2/150 Training Loss: 0.8207915245052875\n",
      "Epoch: 3/150 Training Loss: 0.8119374765563256\n",
      "Epoch: 4/150 Training Loss: 0.804991803013582\n",
      "Epoch: 5/150 Training Loss: 0.802823967745214\n",
      "Epoch: 6/150 Training Loss: 0.7986696706604712\n",
      "Epoch: 7/150 Training Loss: 0.7968292369465648\n",
      "Epoch: 8/150 Training Loss: 0.7964726418154346\n",
      "Epoch: 9/150 Training Loss: 0.7951918908001221\n",
      "Epoch: 10/150 Training Loss: 0.7998132722074633\n",
      "Epoch: 11/150 Training Loss: 0.7991607889686663\n",
      "Epoch: 12/150 Training Loss: 0.796158707633461\n",
      "Epoch: 13/150 Training Loss: 0.7954514456778458\n",
      "Epoch: 14/150 Training Loss: 0.7974489466431215\n",
      "Epoch: 15/150 Training Loss: 0.7948390607981338\n",
      "Epoch: 16/150 Training Loss: 0.7939183222469186\n",
      "Epoch: 17/150 Training Loss: 0.7931243775226816\n",
      "Epoch: 18/150 Training Loss: 0.7918147876909918\n",
      "Epoch: 19/150 Training Loss: 0.7912806768187952\n",
      "Epoch: 20/150 Training Loss: 0.7935996999855304\n",
      "Epoch: 21/150 Training Loss: 0.79264059488716\n",
      "Epoch: 22/150 Training Loss: 0.7927014516391295\n",
      "Epoch: 23/150 Training Loss: 0.7913651150936114\n",
      "Epoch: 24/150 Training Loss: 0.7908447259889845\n",
      "Epoch: 25/150 Training Loss: 0.7894183671761215\n",
      "Epoch: 26/150 Training Loss: 0.7883918541403571\n",
      "Epoch: 27/150 Training Loss: 0.7908850867314028\n",
      "Epoch: 28/150 Training Loss: 0.7949546182278505\n",
      "Epoch: 29/150 Training Loss: 0.7903315699387252\n",
      "Epoch: 30/150 Training Loss: 0.7896459561443001\n",
      "Epoch: 31/150 Training Loss: 0.7886518766380257\n",
      "Epoch: 32/150 Training Loss: 0.7881820787678879\n",
      "Epoch: 33/150 Training Loss: 0.7873904309731579\n",
      "Epoch: 34/150 Training Loss: 0.7868369832071652\n",
      "Epoch: 35/150 Training Loss: 0.7872668963527352\n",
      "Epoch: 36/150 Training Loss: 0.7864852043771252\n",
      "Epoch: 37/150 Training Loss: 0.788312413643316\n",
      "Epoch: 38/150 Training Loss: 0.7881542296753716\n",
      "Epoch: 39/150 Training Loss: 0.7898526986440023\n",
      "Epoch: 40/150 Training Loss: 0.7866262535458988\n",
      "Epoch: 41/150 Training Loss: 0.7859327987706948\n",
      "Epoch: 42/150 Training Loss: 0.7857367050197116\n",
      "Epoch: 43/150 Training Loss: 0.7896354765826484\n",
      "Epoch: 44/150 Training Loss: 0.7896156884550639\n",
      "Epoch: 45/150 Training Loss: 0.7882868999877746\n",
      "Epoch: 46/150 Training Loss: 0.7871978284976736\n",
      "Epoch: 47/150 Training Loss: 0.7876736058811962\n",
      "Epoch: 48/150 Training Loss: 0.7883397074499491\n",
      "Epoch: 49/150 Training Loss: 0.7871720176382163\n",
      "Epoch: 50/150 Training Loss: 0.7838737300990783\n",
      "Epoch: 51/150 Training Loss: 0.782572051708641\n",
      "Epoch: 52/150 Training Loss: 0.7850022467550953\n",
      "Epoch: 53/150 Training Loss: 0.7840629506766591\n",
      "Epoch: 54/150 Training Loss: 0.7824494109530629\n",
      "Epoch: 55/150 Training Loss: 0.7823453686900974\n",
      "Epoch: 56/150 Training Loss: 0.7810548988404552\n",
      "Epoch: 57/150 Training Loss: 0.7817988917999661\n",
      "Epoch: 58/150 Training Loss: 0.7847729730851871\n",
      "Epoch: 59/150 Training Loss: 0.7823694491714137\n",
      "Epoch: 60/150 Training Loss: 0.7820162263113198\n",
      "Epoch: 61/150 Training Loss: 0.7822311987581941\n",
      "Epoch: 62/150 Training Loss: 0.7821073951999756\n",
      "Epoch: 63/150 Training Loss: 0.7842020515314082\n",
      "Epoch: 64/150 Training Loss: 0.78533941876028\n",
      "Epoch: 65/150 Training Loss: 0.7826858185410909\n",
      "Epoch: 66/150 Training Loss: 0.7838351275912675\n",
      "Epoch: 67/150 Training Loss: 0.785717102875005\n",
      "Epoch: 68/150 Training Loss: 0.7881887401502157\n",
      "Epoch: 69/150 Training Loss: 0.7868259499982461\n",
      "Epoch: 70/150 Training Loss: 0.7856198950731468\n",
      "Epoch: 71/150 Training Loss: 0.7832278644096401\n",
      "Epoch: 72/150 Training Loss: 0.7814976790106993\n",
      "Epoch: 73/150 Training Loss: 0.7807016761851884\n",
      "Epoch: 74/150 Training Loss: 0.7797881657724938\n",
      "Epoch: 75/150 Training Loss: 0.7804591350539034\n",
      "Epoch: 76/150 Training Loss: 0.7798207625490693\n",
      "Epoch: 77/150 Training Loss: 0.7806944789755386\n",
      "Epoch: 78/150 Training Loss: 0.7817231744425404\n",
      "Epoch: 79/150 Training Loss: 0.7817749514202892\n",
      "Epoch: 80/150 Training Loss: 0.7849860340868894\n",
      "Epoch: 81/150 Training Loss: 0.7860458540342927\n",
      "Epoch: 82/150 Training Loss: 0.7829107533615479\n",
      "Epoch: 83/150 Training Loss: 0.7813880943350776\n",
      "Epoch: 84/150 Training Loss: 0.7814124793940803\n",
      "Epoch: 85/150 Training Loss: 0.7825105819095861\n",
      "Epoch: 86/150 Training Loss: 0.7951794000425699\n",
      "Epoch: 87/150 Training Loss: 0.7904217884302959\n",
      "Epoch: 88/150 Training Loss: 0.7825098660393679\n",
      "Epoch: 89/150 Training Loss: 0.7811706993997711\n",
      "Epoch: 90/150 Training Loss: 0.780599072217122\n",
      "Epoch: 91/150 Training Loss: 0.7796339277958951\n",
      "Epoch: 92/150 Training Loss: 0.7786839575701973\n",
      "Epoch: 93/150 Training Loss: 0.7796959541097949\n",
      "Epoch: 94/150 Training Loss: 0.7810867832288709\n",
      "Epoch: 95/150 Training Loss: 0.7827792499483246\n",
      "Epoch: 96/150 Training Loss: 0.7799893193638202\n",
      "Epoch: 97/150 Training Loss: 0.7817842034539816\n",
      "Epoch: 98/150 Training Loss: 0.7800191853873918\n",
      "Epoch: 99/150 Training Loss: 0.7786733362682906\n",
      "Epoch: 100/150 Training Loss: 0.7782130884550691\n",
      "Epoch: 101/150 Training Loss: 0.7797931533089209\n",
      "Epoch: 102/150 Training Loss: 0.7813979530662196\n",
      "Epoch: 103/150 Training Loss: 0.781477621740492\n",
      "Epoch: 104/150 Training Loss: 0.7821349745353883\n",
      "Epoch: 105/150 Training Loss: 0.7823353915689737\n",
      "Epoch: 106/150 Training Loss: 0.7820675145719469\n",
      "Epoch: 107/150 Training Loss: 0.7832888116951251\n",
      "Epoch: 108/150 Training Loss: 0.7792748264021071\n",
      "Epoch: 109/150 Training Loss: 0.7793796201751814\n",
      "Epoch: 110/150 Training Loss: 0.7784803380671236\n",
      "Epoch: 111/150 Training Loss: 0.7781409180860749\n",
      "Epoch: 112/150 Training Loss: 0.7787408087261763\n",
      "Epoch: 113/150 Training Loss: 0.7797248107461175\n",
      "Epoch: 114/150 Training Loss: 0.7799838865335864\n",
      "Epoch: 115/150 Training Loss: 0.780663212754882\n",
      "Epoch: 116/150 Training Loss: 0.7792617848648649\n",
      "Epoch: 117/150 Training Loss: 0.7793358367742952\n",
      "Epoch: 118/150 Training Loss: 0.7823964355327829\n",
      "Epoch: 119/150 Training Loss: 0.7805391175230754\n",
      "Epoch: 120/150 Training Loss: 0.7801236903544554\n",
      "Epoch: 121/150 Training Loss: 0.7936552465986141\n",
      "Epoch: 122/150 Training Loss: 0.7811995562409207\n",
      "Epoch: 123/150 Training Loss: 0.7795011261894121\n",
      "Epoch: 124/150 Training Loss: 0.7789281276902792\n",
      "Epoch: 125/150 Training Loss: 0.7786223802369895\n",
      "Epoch: 126/150 Training Loss: 0.7832695110557005\n",
      "Epoch: 127/150 Training Loss: 0.7812866709895969\n",
      "Epoch: 128/150 Training Loss: 0.7795800214370912\n",
      "Epoch: 129/150 Training Loss: 0.7813109438034267\n",
      "Epoch: 130/150 Training Loss: 0.7800550766007597\n",
      "Epoch: 131/150 Training Loss: 0.7790843336442902\n",
      "Epoch: 132/150 Training Loss: 0.7784168992665216\n",
      "Epoch: 133/150 Training Loss: 0.7794807780649244\n",
      "Epoch: 134/150 Training Loss: 0.7809323643900684\n",
      "Epoch: 135/150 Training Loss: 0.7791919392818438\n",
      "Epoch: 136/150 Training Loss: 0.7811546866426763\n",
      "Epoch: 137/150 Training Loss: 0.7796369387521777\n",
      "Epoch: 138/150 Training Loss: 0.783615138932192\n",
      "Epoch: 139/150 Training Loss: 0.7805551775951975\n",
      "Epoch: 140/150 Training Loss: 0.7821490328336499\n",
      "Epoch: 141/150 Training Loss: 0.7796385230886977\n",
      "Epoch: 142/150 Training Loss: 0.7795564126722592\n",
      "Epoch: 143/150 Training Loss: 0.7809055082986445\n",
      "Epoch: 144/150 Training Loss: 0.7791812274464217\n",
      "Epoch: 145/150 Training Loss: 0.7805346750311836\n",
      "Epoch: 146/150 Training Loss: 0.7806442361517051\n",
      "Epoch: 147/150 Training Loss: 0.7774391393481251\n",
      "Epoch: 148/150 Training Loss: 0.7763166243268043\n",
      "Epoch: 149/150 Training Loss: 0.7770020599217758\n",
      "\n",
      "Saving the model file...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the plot at  ..\\..\\data\\loss\\proxy_interp__gru_training_loss_e_150_l_0.01_20210505_165449.png\n",
      "RNNModule(\n",
      "  (embedding): Embedding(242, 48)\n",
      "  (gru): GRU(48, 48, batch_first=True)\n",
      "  (dense): Linear(in_features=48, out_features=242, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if _platform == \"win32\":\n",
    "    data_file_dir = '..\\..\\data\\csv\\\\'\n",
    "    model_file_path = '..\\..\\data\\model_gru\\\\' \n",
    "    loss_curve_path = '..\\..\\data\\loss\\\\'\n",
    "else:\n",
    "    data_file_dir = '../../data/csv/'\n",
    "    model_file_path = '../../data/model_gru/'\n",
    "    loss_curve_path = '../../data/loss/'\n",
    "\n",
    "train_path = fileutils.get_latest_file_with_path(data_file_dir, 'lipdverse_downsampled_*.csv')\n",
    "\n",
    "flags = Namespace(\n",
    "    train_file = train_path,\n",
    "    seq_size_u=3,\n",
    "    seq_size=6,\n",
    "    batch_size=48,\n",
    "    embedding_size=48,\n",
    "    hidden_size=48,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['MarineSediment'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path=model_file_path,\n",
    ")\n",
    "\n",
    "new_list = []\n",
    "reference_dict = {}\n",
    "int_to_vocab = {}\n",
    "vocab_to_int = {}\n",
    "n_vocab_u = 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "living-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of len {'1': 14, '2': 112, '3': 147, '3_units': 160, '4': 243, '5': 459, '6': 465}\n",
      "1563\n",
      "\n",
      "Training Data for the chain archiveType -> proxyObservationType -> proxyObservationTypeUnits\n",
      "\n",
      "Epoch: 0/150 Training Loss: 1.5252051754915428\n",
      "Epoch: 1/150 Training Loss: 1.3548449470415147\n",
      "Epoch: 2/150 Training Loss: 1.349139304095527\n",
      "Epoch: 3/150 Training Loss: 1.3462631219031476\n",
      "Epoch: 4/150 Training Loss: 1.3431905180318249\n",
      "Epoch: 5/150 Training Loss: 1.3421838861150839\n",
      "Epoch: 6/150 Training Loss: 1.340576050617441\n",
      "Epoch: 7/150 Training Loss: 1.3396003860788248\n",
      "Epoch: 8/150 Training Loss: 1.338560393585782\n",
      "Epoch: 9/150 Training Loss: 1.335728787474616\n",
      "Epoch: 10/150 Training Loss: 1.3343024294810606\n",
      "Epoch: 11/150 Training Loss: 1.332065116908542\n",
      "Epoch: 12/150 Training Loss: 1.3304102334779562\n",
      "Epoch: 13/150 Training Loss: 1.3283537409149904\n",
      "Epoch: 14/150 Training Loss: 1.326468889246282\n",
      "Epoch: 15/150 Training Loss: 1.327842214672836\n",
      "Epoch: 16/150 Training Loss: 1.3261798509617442\n",
      "Epoch: 17/150 Training Loss: 1.3252638262981402\n",
      "Epoch: 18/150 Training Loss: 1.322202596058141\n",
      "Epoch: 19/150 Training Loss: 1.3214900763993411\n",
      "Epoch: 20/150 Training Loss: 1.3217801793744064\n",
      "Epoch: 21/150 Training Loss: 1.3241109475237398\n",
      "Epoch: 22/150 Training Loss: 1.3227200848130427\n",
      "Epoch: 23/150 Training Loss: 1.3202944948910849\n",
      "Epoch: 24/150 Training Loss: 1.3213936389516718\n",
      "Epoch: 25/150 Training Loss: 1.3205576475543255\n",
      "Epoch: 26/150 Training Loss: 1.3205373242138998\n",
      "Epoch: 27/150 Training Loss: 1.3199952328737659\n",
      "Epoch: 28/150 Training Loss: 1.319388494049151\n",
      "Epoch: 29/150 Training Loss: 1.318559764586773\n",
      "Epoch: 30/150 Training Loss: 1.317824491520518\n",
      "Epoch: 31/150 Training Loss: 1.320767756999563\n",
      "Epoch: 32/150 Training Loss: 1.3214667105592812\n",
      "Epoch: 33/150 Training Loss: 1.32072368520232\n",
      "Epoch: 34/150 Training Loss: 1.3200782308054133\n",
      "Epoch: 35/150 Training Loss: 1.316165229708878\n",
      "Epoch: 36/150 Training Loss: 1.3159818350244634\n",
      "Epoch: 37/150 Training Loss: 1.316693887677799\n",
      "Epoch: 38/150 Training Loss: 1.3155953515436232\n",
      "Epoch: 39/150 Training Loss: 1.3175900404395926\n",
      "Epoch: 40/150 Training Loss: 1.315724424480163\n",
      "Epoch: 41/150 Training Loss: 1.315544654413597\n",
      "Epoch: 42/150 Training Loss: 1.315259786405924\n",
      "Epoch: 43/150 Training Loss: 1.313175072784686\n",
      "Epoch: 44/150 Training Loss: 1.3122216868646366\n",
      "Epoch: 45/150 Training Loss: 1.3145429387535017\n",
      "Epoch: 46/150 Training Loss: 1.3149195106578446\n",
      "Epoch: 47/150 Training Loss: 1.3170325477508336\n",
      "Epoch: 48/150 Training Loss: 1.3139048507533122\n",
      "Epoch: 49/150 Training Loss: 1.314665986909899\n",
      "Epoch: 50/150 Training Loss: 1.3139642903075595\n",
      "Epoch: 51/150 Training Loss: 1.3130458851450497\n",
      "Epoch: 52/150 Training Loss: 1.3227085088126849\n",
      "Epoch: 53/150 Training Loss: 1.3161471627422214\n",
      "Epoch: 54/150 Training Loss: 1.3152439467276085\n",
      "Epoch: 55/150 Training Loss: 1.3139163354008467\n",
      "Epoch: 56/150 Training Loss: 1.3129427936888232\n",
      "Epoch: 57/150 Training Loss: 1.3124578494796229\n",
      "Epoch: 58/150 Training Loss: 1.3131424803094767\n",
      "Epoch: 59/150 Training Loss: 1.3154709621803047\n",
      "Epoch: 60/150 Training Loss: 1.311404360528664\n",
      "Epoch: 61/150 Training Loss: 1.3172596754487027\n",
      "Epoch: 62/150 Training Loss: 1.3181948600356113\n",
      "Epoch: 63/150 Training Loss: 1.3137836542326151\n",
      "Epoch: 64/150 Training Loss: 1.3123078100460093\n",
      "Epoch: 65/150 Training Loss: 1.312918625336742\n",
      "Epoch: 66/150 Training Loss: 1.3107488704301238\n",
      "Epoch: 67/150 Training Loss: 1.31131528087498\n",
      "Epoch: 68/150 Training Loss: 1.3130466520171804\n",
      "Epoch: 69/150 Training Loss: 1.3136913190592605\n",
      "Epoch: 70/150 Training Loss: 1.3153251491461422\n",
      "Epoch: 71/150 Training Loss: 1.3100561915394366\n",
      "Epoch: 72/150 Training Loss: 1.3093362231434826\n",
      "Epoch: 73/150 Training Loss: 1.312067322714632\n",
      "Epoch: 74/150 Training Loss: 1.3112051794209432\n",
      "Epoch: 75/150 Training Loss: 1.3126495339206814\n",
      "Epoch: 76/150 Training Loss: 1.3144079614750708\n",
      "Epoch: 77/150 Training Loss: 1.3114547168266322\n",
      "Epoch: 78/150 Training Loss: 1.3121226538497557\n",
      "Epoch: 79/150 Training Loss: 1.3095261612298972\n",
      "Epoch: 80/150 Training Loss: 1.308157489062175\n",
      "Epoch: 81/150 Training Loss: 1.3105190480287952\n",
      "Epoch: 82/150 Training Loss: 1.3109094568134583\n",
      "Epoch: 83/150 Training Loss: 1.3087631607383388\n",
      "Epoch: 84/150 Training Loss: 1.3057748820773514\n",
      "Epoch: 85/150 Training Loss: 1.3064588820401746\n",
      "Epoch: 86/150 Training Loss: 1.3075652544441092\n",
      "Epoch: 87/150 Training Loss: 1.3056420546626717\n",
      "Epoch: 88/150 Training Loss: 1.3100747689348726\n",
      "Epoch: 89/150 Training Loss: 1.311089497251609\n",
      "Epoch: 90/150 Training Loss: 1.3107948270450342\n",
      "Epoch: 91/150 Training Loss: 1.3106384011068704\n",
      "Epoch: 92/150 Training Loss: 1.3092452181163932\n",
      "Epoch: 93/150 Training Loss: 1.3103214502334595\n",
      "Epoch: 94/150 Training Loss: 1.3085135315701724\n",
      "Epoch: 95/150 Training Loss: 1.3099362510176458\n",
      "Epoch: 96/150 Training Loss: 1.30898062470033\n",
      "Epoch: 97/150 Training Loss: 1.3090448785073978\n",
      "Epoch: 98/150 Training Loss: 1.3070632764154284\n",
      "Epoch: 99/150 Training Loss: 1.306305868109477\n",
      "Epoch: 100/150 Training Loss: 1.3072675671364433\n",
      "Epoch: 101/150 Training Loss: 1.310808417723351\n",
      "Epoch: 102/150 Training Loss: 1.3061089511589496\n",
      "Epoch: 103/150 Training Loss: 1.3089191344185793\n",
      "Epoch: 104/150 Training Loss: 1.3084372581075556\n",
      "Epoch: 105/150 Training Loss: 1.3066421968420756\n",
      "Epoch: 106/150 Training Loss: 1.3066520793331449\n",
      "Epoch: 107/150 Training Loss: 1.3064221954837287\n",
      "Epoch: 108/150 Training Loss: 1.3097007823563933\n",
      "Epoch: 109/150 Training Loss: 1.3086719119671695\n",
      "Epoch: 110/150 Training Loss: 1.3070357329247333\n",
      "Epoch: 111/150 Training Loss: 1.3080562631698818\n",
      "Epoch: 112/150 Training Loss: 1.3054730867602162\n",
      "Epoch: 113/150 Training Loss: 1.308711291588459\n",
      "Epoch: 114/150 Training Loss: 1.308467603221382\n",
      "Epoch: 115/150 Training Loss: 1.3092611643047267\n",
      "Epoch: 116/150 Training Loss: 1.3085965766120202\n",
      "Epoch: 117/150 Training Loss: 1.312210422201255\n",
      "Epoch: 118/150 Training Loss: 1.311053352667294\n",
      "Epoch: 119/150 Training Loss: 1.31038528168734\n",
      "Epoch: 120/150 Training Loss: 1.307137239020305\n",
      "Epoch: 121/150 Training Loss: 1.3061065223208816\n",
      "Epoch: 122/150 Training Loss: 1.3083040947766649\n",
      "Epoch: 123/150 Training Loss: 1.3058494288487124\n",
      "Epoch: 124/150 Training Loss: 1.3055424686150043\n",
      "Epoch: 125/150 Training Loss: 1.3086869581458496\n",
      "Epoch: 126/150 Training Loss: 1.3088813131207864\n",
      "Epoch: 127/150 Training Loss: 1.30863119646446\n",
      "Epoch: 128/150 Training Loss: 1.3096403366921283\n",
      "Epoch: 129/150 Training Loss: 1.3095231273329955\n",
      "Epoch: 130/150 Training Loss: 1.312687366688784\n",
      "Epoch: 131/150 Training Loss: 1.310562683134964\n",
      "Epoch: 132/150 Training Loss: 1.3089661913639081\n",
      "Epoch: 133/150 Training Loss: 1.3094640622024274\n",
      "Epoch: 134/150 Training Loss: 1.308301172715282\n",
      "Epoch: 135/150 Training Loss: 1.3083834631746167\n",
      "Epoch: 136/150 Training Loss: 1.3104622191989546\n",
      "Epoch: 137/150 Training Loss: 1.3089222580296886\n",
      "Epoch: 138/150 Training Loss: 1.3056660190071028\n",
      "Epoch: 139/150 Training Loss: 1.306650371895623\n",
      "Epoch: 140/150 Training Loss: 1.3071469670718479\n",
      "Epoch: 141/150 Training Loss: 1.307735025677894\n",
      "Epoch: 142/150 Training Loss: 1.3072639490730573\n",
      "Epoch: 143/150 Training Loss: 1.3051343357440122\n",
      "Epoch: 144/150 Training Loss: 1.3031157808205516\n",
      "Epoch: 145/150 Training Loss: 1.303382738759018\n",
      "Epoch: 146/150 Training Loss: 1.3035342001013739\n",
      "Epoch: 147/150 Training Loss: 1.303882404291343\n",
      "Epoch: 148/150 Training Loss: 1.3036578819923794\n",
      "Epoch: 149/150 Training Loss: 1.3030525667151225\n",
      "\n",
      "Saving the model file...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the plot at  ..\\..\\data\\loss\\proxy_units__gru_training_loss_e_150_l_0.01_20210505_170255.png\n",
      "RNNModule(\n",
      "  (embedding): Embedding(123, 48)\n",
      "  (gru): GRU(48, 48, batch_first=True)\n",
      "  (dense): Linear(in_features=48, out_features=123, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-smart",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
