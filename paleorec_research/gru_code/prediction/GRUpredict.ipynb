{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "focused-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Feb 18 12:13:12 2021\n",
    "\n",
    "@author: shrav\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weighted-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, hidden_size):\n",
    "        \n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size,\n",
    "                            hidden_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_size, n_vocab)\n",
    "    \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.gru(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "#         print('zero_state called')\n",
    "        return Variable(torch.zeros(1,batch_size,self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adolescent-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_file_with_path(path, *paths):\n",
    "    '''\n",
    "    Method to get the full path name for the latest file for the input parameter in paths.\n",
    "    This method uses the os.path.getctime function to get the most recently created file that matches the filename pattern in the provided path. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Root pathname for the files.\n",
    "    *paths : string list\n",
    "        These are the var args field, the optional set of strings to denote the full path to the file names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    latest_file : string\n",
    "        Full path name for the latest file provided in the paths parameter.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    fullpath = os.path.join(path, *paths)\n",
    "    list_of_files = glob.iglob(fullpath)  \n",
    "    if not list_of_files:                \n",
    "        return None\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "featured-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUpredict:\n",
    "        \n",
    "    def __init__(self, model_file_path, ground_truth_file_path, topk):\n",
    "        \n",
    "        flags = Namespace(\n",
    "            seq_size_u=3,\n",
    "            seq_size=6,\n",
    "            batch_size=48,\n",
    "            embedding_size=48,\n",
    "            lstm_size=48,\n",
    "            gradients_norm=5,\n",
    "            initial_words=['MarineSediment'],\n",
    "            predict_top_k=5,\n",
    "            checkpoint_path=model_file_path,\n",
    "        )\n",
    "        # PATH for model file\n",
    "        PATH = get_latest_file_with_path(model_file_path, 'model_gru_interp_*.pth')\n",
    "        PATH_UNITS = get_latest_file_with_path(model_file_path, 'model_gru_units_*.pth')\n",
    "        MODEL_TOKEN_INFO_PATH = get_latest_file_with_path(model_file_path, 'model_token_info_*.txt')\n",
    "        MODEL_TOKEN_UNITS_INFO_PATH = get_latest_file_with_path(model_file_path, 'model_token_units_info_*.txt')\n",
    "        GROUND_TRUTH_FILE_PATH = get_latest_file_with_path(ground_truth_file_path, 'ground_truth_label_*.json')\n",
    "            \n",
    "        # Initialize device to load model onto\n",
    "        # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.device = torch.device('cpu')\n",
    "        self.topk = topk\n",
    "        \n",
    "        # Read token info \n",
    "        with open(MODEL_TOKEN_INFO_PATH, 'r') as json_file:\n",
    "            self.model_tokens = json.load(json_file)\n",
    "        \n",
    "        self.int_to_vocab = self.model_tokens['model_tokens']\n",
    "        self.int_to_vocab = {int(k):v for k,v in self.int_to_vocab.items()}\n",
    "        self.vocab_to_int = {v:k for k,v in self.int_to_vocab.items()}\n",
    "        n_vocab = len(self.int_to_vocab)\n",
    "        \n",
    "        self.reference_dict = self.model_tokens['reference_dict']\n",
    "        self.reference_dict_val = set(self.reference_dict.values())\n",
    "\n",
    "        self.len_dict = self.model_tokens['len_dict']\n",
    "        \n",
    "        with open(MODEL_TOKEN_UNITS_INFO_PATH, 'r') as json_file:\n",
    "            self.model_tokens = json.load(json_file)\n",
    "            \n",
    "        self.int_to_vocab_u = self.model_tokens['model_tokens_u']\n",
    "        self.int_to_vocab_u = {int(k):v for k,v in self.int_to_vocab_u.items()}\n",
    "        self.vocab_to_int_u = {v:k for k,v in self.int_to_vocab_u.items()}\n",
    "        n_vocab_u = len(self.int_to_vocab_u)\n",
    "        \n",
    "        self.reference_dict_u = self.model_tokens['reference_dict_u']\n",
    "        \n",
    "        # Initialize the model for archive -> proxyObservationType -> interpretation/variable -> \n",
    "        #                                         interpretation/variableDetail -> inferredVariable -> inferredVarUnits\n",
    "        self.model = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
    "        self.model.load_state_dict(torch.load(PATH, map_location=self.device), strict=False)\n",
    "        \n",
    "        \n",
    "        # Initialize the model for archive -> proxyObservationType -> units\n",
    "        self.model_u = RNNModule(n_vocab_u, flags.seq_size_u, flags.embedding_size, flags.lstm_size)\n",
    "        self.model_u.load_state_dict(torch.load(PATH_UNITS, map_location=self.device), strict=False)\n",
    "        \n",
    "        # Read file to get category names list information\n",
    "        with open(GROUND_TRUTH_FILE_PATH, 'r') as f:\n",
    "            self.ground_truth = json.load(f)    \n",
    "            \n",
    "        self.names_set = {0 : set(self.ground_truth['archive_types']), 1: set(self.ground_truth['proxy_obs_types']), \n",
    "                      2: set(self.ground_truth['units']), 3: set(self.ground_truth['int_var']), 4: set(self.ground_truth['int_var_det']), \n",
    "                      5: set(self.ground_truth['inf_var']), 6: set(self.ground_truth['inf_var_units'])}\n",
    "        for i in range(6):\n",
    "            self.names_set[i] = {val.replace(' ', '') for val in self.names_set[i]}\n",
    "        \n",
    "        self.archives_map = self.ground_truth['archives_map']\n",
    "\n",
    "    \n",
    "    def predict(self, device, net, words, vocab_to_int, int_to_vocab, names_set):\n",
    "        '''\n",
    "        Returns the list of top 5 predictions for the provided list of words using the model stored in net.\n",
    "        The device is initialized to CPU for the purpose of the predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        device : torch.device\n",
    "            Device type to signify 'cpu' or 'gpu'.\n",
    "        net : torch.module\n",
    "            Instance of LSTM created using RNN Module.\n",
    "        words : list\n",
    "            List of strings used for predicting the next string in the list of words.\n",
    "        vocab_to_int : dict\n",
    "            Mapping of strings to int used to embed the input strings.\n",
    "        int_to_vocab : dict\n",
    "            Mapping of int to string used in the process of running the model predictions.\n",
    "        names_set : dict\n",
    "            Mapping of fieldType(example proxyObsType, interpretation/variable) and list of all the possible values the field can take.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            Top 5 recommendations for the next string in the sequence of words.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        net.eval()\n",
    "        top_k = 10\n",
    "        if words[-1] in self.ground_truth['ground_truth']:\n",
    "            top_k = len(self.ground_truth['ground_truth'][words[-1]])\n",
    "        state_h = net.zero_state(1)\n",
    "        state_h = state_h.to(device)\n",
    "        \n",
    "        for w in words:\n",
    "            if w not in vocab_to_int:\n",
    "                return []\n",
    "            ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "            output, state_h = net(ix, state_h)\n",
    "        \n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        \n",
    "        output = []\n",
    "        for val in choices[0]:\n",
    "            name = int_to_vocab[val]\n",
    "            if name in names_set:\n",
    "                output.append(name)\n",
    "            # if len(output) == self.topk:\n",
    "                # break\n",
    "            if len(output) == top_k:\n",
    "                break\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def predictForSentence(self, sentence, isInferred = False):\n",
    "        '''\n",
    "        This method is used from the Flask Server Code API. \n",
    "        This method handles the initialization of the lstm model for the two different prediction chains we are using in our system.\n",
    "        \n",
    "        archiveType -> proxyObservationType -> units\n",
    "        archiveType -> proxyObservationType -> interpretation/variable -> interpretation/variableDetial -> inferredVariable -> inferredVarUnits\n",
    "        \n",
    "        Depending on the length of the input sentence and the variableType, it chooses the output that will be returned to the server.\n",
    "        \n",
    "        If the variableType == measured\n",
    "        then we will be considering the complete chain for prediction\n",
    "        example: If sentence length = 1, it contains the archiveType and output = prediction for proxyObservationType\n",
    "        example: If sentence length = 2, it contains the archiveType and proxyObservationType and output = units and interpretation/variable\n",
    "        so on..\n",
    "        \n",
    "        If the variableType == inferred\n",
    "        then we will be considering the top attributes in the chain from proxyObservationType to interpretation/variableDetail \n",
    "        to predict the top 5 inferredVariable as the output .\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence : string\n",
    "            Input sentence to predict the next field.\n",
    "        isInferred : boolean, optional\n",
    "            True if variableType == 'inferred'. The default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Contains the result list of predictions as the value.\n",
    "            Depending on the length of the input sentence and the variableType,\n",
    "            the dict can contain one item corresponding to key '0' or two items corresponding to the two keys '0' and '1'.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        input_sent_list = sentence.strip().split(',')\n",
    "        input_sent_list = [val.replace(' ', '') for val in input_sent_list]\n",
    "        input_sent_list = [self.reference_dict.get(val, val) for val in input_sent_list]\n",
    "\n",
    "        if isInferred and len(input_sent_list) <= 2:\n",
    "            \n",
    "            inferredVar = None\n",
    "            if len(input_sent_list) == 2:\n",
    "                inferredVar = input_sent_list[1]\n",
    "                if inferredVar not in self.names_set[5]:\n",
    "                    return {'0': []}\n",
    "                del input_sent_list[1]\n",
    "            while(len(input_sent_list) < 4):\n",
    "                sentence = (',').join(input_sent_list)\n",
    "                if len(input_sent_list) == 2:\n",
    "                    top_lstm_pred_res = self.predictForSentence(sentence)\n",
    "                    if not top_lstm_pred_res['1']:\n",
    "                        return {'0': []}\n",
    "                    input_sent_list.append(top_lstm_pred_res['1'][0])\n",
    "                else:\n",
    "                    top_lstm_pred_res = self.predictForSentence(sentence)\n",
    "                    if not top_lstm_pred_res['0']:\n",
    "                        return {'0': []}\n",
    "                    input_sent_list.append(top_lstm_pred_res['0'][0])\n",
    "                    \n",
    "                \n",
    "            if inferredVar:\n",
    "                input_sent_list.append(inferredVar)\n",
    "            names_set_ind = len(input_sent_list) + 1 if len(input_sent_list) >= 2 else len(input_sent_list)\n",
    "            results = self.predict(self.device, self.model, input_sent_list, self.vocab_to_int, self.int_to_vocab, self.names_set[names_set_ind])\n",
    "            return {'0':results}\n",
    "        \n",
    "        \n",
    "        names_set_ind = len(input_sent_list) + 1 if len(input_sent_list) >= 2 else len(input_sent_list)\n",
    "        if len(input_sent_list) == 2:\n",
    "            # print('input sent len 2', input_sent_list)\n",
    "            results_units =  self.predict(self.device, self.model_u, input_sent_list, self.vocab_to_int_u, self.int_to_vocab_u, self.names_set[len(input_sent_list)])\n",
    "            results = self.predict(self.device, self.model, input_sent_list, self.vocab_to_int, self.int_to_vocab, self.names_set[names_set_ind])\n",
    "            return {'0':results_units, '1':results}\n",
    "        else:\n",
    "            # print('input sent len not 2', input_sent_list)\n",
    "            results = self.predict(self.device, self.model, input_sent_list, self.vocab_to_int, self.int_to_vocab, self.names_set[names_set_ind])\n",
    "            return {'0':results}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
